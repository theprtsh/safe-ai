Which of the following is true?
Extremistan has thin tails while Mediocristan has long tails
Mediocristan distributions are harder to predict than Extremistan
In Extremistan, the total is determined by a few large events with tyranny of the accidental
Extremistan has mild randomness while Mediocristan has wild randomness

---
What is the key difference between covariate shift and concept shift in distribution shifts?
Covariate shift changes P(y|x) while concept shift changes P(x)
Covariate shift changes P(x) while P(y|x) remains constant, concept shift changes P(y|x) while P(x) remains constant
Both change P(x) and P(y|x) simultaneously
Covariate shift affects labels while concept shift affects features

---
In the AugMix methodology, what is the primary advantage over uncontrolled random augmentations?
It uses skip connections to keep images recognizable while applying diverse augmentations
It requires less computational power
It only applies single augmentations instead of multiple
It focuses on geometric transformations only

---
In the context of adversarial attacks, what does "transferability" specifically refer to?
The ability to transfer attacks from one domain to another
The ability to transfer defenses across different architectures
The ability to convert white-box attacks to black-box attacks
The ability of adversarial examples crafted for one model to work on other models

---
Black Swan lies in which of the following categories?
Known Knowns
Known Unknowns
Unknown Knowns
Unknown Unknowns

---
Which of the following are valid approaches for defending against adversarial attacks? (Select all that apply)
Data augmentation techniques
Adversarial training using adversarial examples during training
Using more data and larger models
Reducing model complexity to avoid overfitting
Adversarial pretraining on larger datasets like ImageNet

---
In the RLHF optimization objective, why is a KL-divergence penalty term added to the reward maximization?
To prevent the model from generating repetitive outputs
To ensure the model stays close to the original pretrained model
To improve the computational efficiency of the training process
To increase the diversity of generated samples

---
What does "reward hacking" specifically refer to in the context of RLHF?
Humans providing incorrect feedback to manipulate the system
External attackers compromising the reward model
The reward model overfitting to the training data
The model finding ways to maximize the reward function without achieving the intended behavior

---
Identify the equations that can lead to a long-tailed distribution.
Idea * student * resources * time
Idea * student + resources * time
Idea + student + resource + time
Idea - student * resource - time

---
What is the primary advantage of using pairwise comparisons over direct scalar ratings in human feedback collection?
Pairwise comparisons are faster to collect
They require fewer human annotators
Human judgments are noisy and miscalibrated, but pairwise comparisons are more reliable
They provide more granular feedback information

---
What is a major challenge with using a single reward function in RLHF?
It is computationally expensive to optimize
It cannot represent a diverse society of humans
It requires too much training data
It is unstable during training

---
What is Direct Preference Optimization (DPO) equivalent to in the context of RLHF component removal?
RLHF - Human Feedback
RLHF - Reward Model
RLHF - RL
RLHF - Policy Optimization
