As per the lecture, in the context of Machine Learning, what is the definition of ‘bias’?
Systematic deviation from rationality and judgement.
Systematic error in the collection, analysis, or interpretation of data.
Systematic favoritism or discrimination towards certain groups/outcomes.
Systematic behaviour when solving complex tasks.

---
Why did Microsoft’s Tay chatbot become offensive shortly after it was launched?
It learned toxic behaviour from user interactions on Twitter.
It was hacked by a rival company.
It was trained on outdated information.
It was programmed to be controversial for publicity.

---
As per the lecture, which of the following is a/are a category of bias? (Select all that apply.)
Gender
Race
Scientific Facts
Profession
Currency exchange rates

---
According to the ML Pipeline, what may be a source of bias? (Select all that apply.)
Annotators beliefs
Hardware used for computation
Balanced dataset
Biased training data

---
What does the Legal Safety Score (LSSβ) represent?
The model's ability to predict legal outcomes based solely on accuracy.
A metric combining fairness and accuracy using a β-weighted harmonic mean.
A score based on the usage of legal jargon for marginalized groups.
An evaluation metric used to define how well a model understands legal jargon.

---
What do LLMs use to prevent harmful outputs?
Data augmentation
Guardrails
Faster GPUs
Dropout layers

---
Which of the following is true about bias?
It never exists.
It sometimes exists.
It only exists in American-created models.
It always exists.

---
A researcher is evaluating a facial recognition model they helped develop. During testing, they select images where the model performs better, such as images with ideal lighting or frontal faces, while ignoring diverse or difficult cases (like low-light, non-white faces, or side angles). Which type of bias is this?
Reporting bias
Sampling bias
Experimenter’s bias
Historical bias

---
What is the issue with evaluating models only based on accuracy?
Accuracy only reflects hardware performance.
Accuracy doesn’t reveal bias.
Accuracy checks for fairness.
Accuracy changes the labelling.

---
Why might using Western-aligned datasets be problematic for the Indian demographic?
The datasets are too large which may slow down training time.
They are written in a different language.
They don’t reflect Indian social/societal norms.
They contain too many low-resolution images.

---
What is a ‘stereotype’?
A factual statement that applies to all humans.
A scientifically proven characteristic.
A legal rule used to govern a society.
A widely held belief about some group/entity.

---
What does the CrowS-Pairs dataset contain?
Pairs of sentences that differ only in minimally distant social bias.
Dialogues between humans and a chatbot.
Pairs of biased and unbiased images.
Code snippets with and without bugs.

---
What is sampling bias?
When historical data reflects inequalities that existed in the world at that time.
When data is collected from a completely random and diverse group.
If proper randomization is not used during data collection.
When a model builder keeps training a model until it produces a result that aligns with their original hypothesis.
You may submit any number of times before the due date. The final submission will be considered for grading.
