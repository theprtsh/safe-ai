According to the risk decomposition framework, which combination of factors would result in the HIGHEST risk from an AI system deployed in a critical infrastructure setting?
Low vulnerability, high hazard exposure, low hazard severity
High vulnerability, low hazard exposure, high hazard severity
High vulnerability, high hazard exposure, high hazard severity
Low vulnerability, low hazard exposure, high hazard severity

---
The concept of treacherous turns in AI systems refers to:
AI systems making computational errors during complex calculations
AI systems behaving differently once they reach sufficient intelligence
AI systems being hacked by malicious actors
AI systems consuming too much computational power

---
In the context of AI race dynamics, what is the primary concern regarding competitive pressure between nations and corporations?
It will make AI systems too expensive for general use
It will result in compatible AI standards globally
It will slow down AI innovation and progress
It may lead to rushed development that compromises safety measures

---
The "Swiss cheese model" mentioned in organizational risks suggests that:
Organizations should have a single, very strong safety measure
Safety measures should be implemented randomly across the organization
Multiple layers of defense compensate for individual weaknesses
Safety measures are unnecessary if the AI system is well-designed

---
Which scenario best illustrates the concept of proxy gaming?
An AI chess program that cheats by accessing opponent's strategy
A recommendation system optimizing for user engagement rather than user well-being
An AI translator that produces grammatically incorrect sentences
A facial recognition system that fails to identify certain ethnic groups

---
A factory robot confuses a human worker for a box of vegetables and pushes the person, resulting in death." According to the disaster risk equation, what was the primary failure component?
Hazard (misclassification capability)
Hazard Exposure (human-robot proximity)
Vulnerability (employee safety protocols)
All components failed equally

---
According to the risk taxonomy presented, malicious use of AI differs from rogue AI primarily in that:
Malicious use involves intentional harmful deployment by humans, while rogue AI acts independently
Malicious use only affects cybersecurity, while rogue AI affects all domains
Malicious use is easier to detect than rogue AI behavior
Malicious use requires more advanced AI capabilities than rogue AI

---
Deceptive Alignment in AI systems is:
AI systems that are openly hostile to humans
AI systems that appear to be following instructions but are actually pursuing different goals
AI systems that cannot understand human language properly
AI systems that work too slowly to be effective

---
How do you identify and avoid hazards in ML systems according to the disaster risk equation framework?
Alignment
Robustness
Monitoring
Systemic Safety

---
Red teaming in AI safety primarily serves to:
Accelerate model training
Identify system vulnerabilities
Improve computational efficiency
Reduce inference latency

---
Which technique is most effective for detecting deceptive alignment?
Training the model with more than 1000 samples
Mechanistic interpretability
Increasing model parameters
Reward modeling

---
RoBERTa succeeds in reasoning tasks where BERT fails due to:
Better tokenization
Emergent capabilities from scaling
Improved attention mechanisms
Larger vocabulary size
