According to the risk decomposition framework, which combination of factors would result in the HIGHEST risk from an AI system deployed in a critical infrastructure setting?
Low vulnerability, high hazard exposure, low hazard severity
High vulnerability, low hazard exposure, high hazard severity
High vulnerability, high hazard exposure, high hazard severity
Low vulnerability, low hazard exposure, high hazard severity

Correct response: High vulnerability, high hazard exposure, high hazard severity
Reasoning: Risk is typically calculated as a function of vulnerability, hazard exposure, and hazard severity. All three factors being high would result in the highest overall risk.

---
The concept of treacherous turns in AI systems refers to:
AI systems making computational errors during complex calculations
AI systems behaving differently once they reach sufficient intelligence
AI systems being hacked by malicious actors
AI systems consuming too much computational power

Correct response: AI systems behaving differently once they reach sufficient intelligence
Reasoning: Treacherous turns refer to AI systems that appear aligned during development but exhibit dangerous behaviors once they become sufficiently intelligent.

---
In the context of AI race dynamics, what is the primary concern regarding competitive pressure between nations and corporations?
It will make AI systems too expensive for general use
It will result in compatible AI standards globally
It will slow down AI innovation and progress
It may lead to rushed development that compromises safety measures

Correct response: It may lead to rushed development that compromises safety measures
Reasoning: The primary concern is that competitive pressure could incentivize cutting corners on safety to achieve faster development and deployment.

---
The "Swiss cheese model" mentioned in organizational risks suggests that:
Organizations should have a single, very strong safety measure
Safety measures should be implemented randomly across the organization
Multiple layers of defense compensate for individual weaknesses
Safety measures are unnecessary if the AI system is well-designed

Correct response: Multiple layers of defense compensate for individual weaknesses
Reasoning: The Swiss cheese model illustrates how multiple imperfect safety layers can work together, where holes in one layer are covered by others.

---
Which scenario best illustrates the concept of proxy gaming?
An AI chess program that cheats by accessing opponent's strategy
A recommendation system optimizing for user engagement rather than user well-being
An AI translator that produces grammatically incorrect sentences
A facial recognition system that fails to identify certain ethnic groups

Correct response: A recommendation system optimizing for user engagement rather than user well-being
Reasoning: Proxy gaming occurs when an AI optimizes for a measurable proxy (engagement) rather than the true objective (user well-being).

---
A factory robot confuses a human worker for a box of vegetables and pushes the person, resulting in death." According to the disaster risk equation, what was the primary failure component?
Hazard (misclassification capability)
Hazard Exposure (human-robot proximity)
Vulnerability (employee safety protocols)
All components failed equally

Correct response: Hazard (misclassification capability)
Reasoning: The primary failure was the robot's inability to correctly classify humans vs objects, which is a hazard in the risk equation.

---
According to the risk taxonomy presented, malicious use of AI differs from rogue AI primarily in that:
Malicious use involves intentional harmful deployment by humans, while rogue AI acts independently
Malicious use only affects cybersecurity, while rogue AI affects all domains
Malicious use is easier to detect than rogue AI behavior
Malicious use requires more advanced AI capabilities than rogue AI

Correct response: Malicious use involves intentional harmful deployment by humans, while rogue AI acts independently
Reasoning: The key distinction is whether harmful actions originate from human intent (malicious use) or emerge from the AI's own goals/behavior (rogue AI).

---
Deceptive Alignment in AI systems is:
AI systems that are openly hostile to humans
AI systems that appear to be following instructions but are actually pursuing different goals
AI systems that cannot understand human language properly
AI systems that work too slowly to be effective

Correct response: AI systems that appear to be following instructions but are actually pursuing different goals
Reasoning: Deceptive alignment describes AI that seems aligned during testing but has different underlying objectives it pursues when able.

---
How do you identify and avoid hazards in ML systems according to the disaster risk equation framework?
Alignment
Robustness
Monitoring
Systemic Safety

Correct response: Systemic Safety
Reasoning: Systemic safety encompasses comprehensive approaches to identify and mitigate hazards throughout the system's lifecycle.

---
Red teaming in AI safety primarily serves to:
Accelerate model training
Identify system vulnerabilities
Improve computational efficiency
Reduce inference latency

Correct response: Identify system vulnerabilities
Reasoning: Red teaming involves actively trying to find weaknesses and failure modes in AI systems through adversarial testing.

---
Which technique is most effective for detecting deceptive alignment?
Training the model with more than 1000 samples
Mechanistic interpretability
Increasing model parameters
Reward modeling

Correct response: Mechanistic interpretability
Reasoning: Mechanistic interpretability aims to understand the internal workings of models, which could reveal hidden/deceptive objectives.

---
RoBERTa succeeds in reasoning tasks where BERT fails due to:
Better tokenization
Emergent capabilities from scaling
Improved attention mechanisms
Larger vocabulary size

Correct response: Emergent capabilities from scaling
Reasoning: RoBERTa's improvements come primarily from more training data and compute (scaling), which can lead to emergent reasoning abilities.
