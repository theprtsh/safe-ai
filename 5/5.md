What is considered an ideal Stereotype Score (ss)?
0%
25%
50%
75%

---
What does SEAT stand for?
Standard Evaluation Assessment Test
Semantic Evaluation Annotation Test
Structured Embedding Accuracy Test
Sentence Embedding Association Test

---
In counterfactual data augmentation (CDA), what is altered to rebalance the corpus?
Sentence Length
Syntex
Bias attribute words
Vocabulary complexity

---
Which characteristic makes a language model more likely to generate gender-neutral responses in text-to-text tasks? (Select all that apply.)
Training on diverse and balanced datasets.
Use of bias-specific adapter modules.
Conditioning outputs on explicit gender tokens.
Reliance on pretrained token embeddings without fine-tuning.

---
Which toolkit is used to add programmable guardrails to LLM-based conversational applications like ChatGPT?
GPT-4
NVIDIA NeMo
CoDi
MAFIA

---
Which of the following is the correct formula for Pointwise Mutual Information (PMI)?
PMI(wi,wj)=log2N⋅c(wi,wj)c(wi)2⋅c(wj)2
PMI(wi,wj)=log2c(wi)⋅c(wj)N⋅c(wi,wj)
PMI(wi,wj)=log2N⋅c(wi,wj)c(wi)⋅c(wj)
PMI(wi,wj)=log2c(wi,wj)N⋅c(wi)⋅c(wj)

---
‘Useful fairness’ couples which of the following? (Select all that apply.)
Context awareness
Bias Score
STS task performance
Dataset diversity

---
What is the key architectural idea behind the MAFIA model for effective debiasing?
Fusing bias-specific adapters while keeping the base model the same.
Replacing all model weights with debiased adapters.
Dynamically routing inputs based on detected bias type.
Using GANs to hallucinate fair outputs.

---
Which of the following is true about proprietary models?
They are more neutral compared to CoDi and other open source models.
They are less neutral compared to CoDi and other open source models.
They have more bias than CoDi and other open source models.
They have the same neutrality as CoDi and other open source models.

---
Which of the following are benchmark datasets commonly used to measure bias in language models? (Select all that apply.)
Stereoset
CrowS-Pairs
ImageNet
Bias-STS-S
SQuAD

---
What is ‘gender-bleaching’ in the context of VLMs?
Improving the quality of input images.
Turning all people in the input images white.
Enhancing gender-specific features in input images.
Removing/Obscuring visual cues related to gender in input images.

---
Why might a single adapter for all bias types (like iDEBall) fail?
Cannot effectively debias across all categories.
Trains slower than other models.
Requires more input data.
Does not understand contextual information.
You may submit any number of times before the due date. The final submission will be considered for grading.

